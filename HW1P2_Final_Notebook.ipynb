{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PSC Users**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ **Step 1 Setting Up Your Environment on Bridges2**\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è For this homework, we are **providing shared Datasets and a shared Conda environment** for the entire class.\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è So for PSC users, **do not download the data yourself** and **do not need to manually install the packages**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:\n",
    "\n",
    "To run your notebook more efficiently on PSC, we need to use a **Jupyter Server** hosted on a compute node.\n",
    "\n",
    "You can use your prefered way of connecting to the Jupyter Server. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The recommended way of connecting is:**\n",
    "\n",
    "#### **Connect in VSCode**\n",
    "SSH into Bridges2 and navigate to your **Jet directory** (`Jet/home/<your_username>`). Upload your notebook there, and then connect to the Jupyter Server from that directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. SSH into Bridges2**\n",
    "1ÔºâOpen VS Code and click on the `Extensions` icon in the left sidebar. Make sure the \"**Remote - SSH**\" extension is installed.\n",
    "\n",
    "2ÔºâOpen the command palette (**Shift+Command+P** on Mac, **Ctrl+Shift+P** on Windows). A search box will appear at the top center. Choose `\"Remote-SSH: Add New SSH Host\"`, then enter:\n",
    "\n",
    "```bash\n",
    "ssh <your_username>@bridges2.psc.edu #change <your_username> to your username\n",
    "```\n",
    "\n",
    "Next, choose `\"/Users/<your_username>/.ssh/config\"` as the config file. A dialog will appear in the bottom right saying \"Host Added\". Click `\"Connect\"`, and then enter your password.\n",
    "\n",
    "(Note: After adding the host once, you can later use `\"Remote-SSH: Connect to Host\"` and select \"bridges2.psc.edu\" from the list.)\n",
    "\n",
    "3ÔºâOnce connected, click `\"Explorer\"` in the left sidebar > \"Open Folder\", and navigate to your home directory under the project grant:\n",
    "```bash\n",
    "/jet/home/<your_username>  #change <your_username> to your username\n",
    "```\n",
    "\n",
    "4ÔºâYou can now drag your notebook files directly into the right-hand pane (your remote home directory), or upload them using `scp` into your folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ùóÔ∏è‚ö†Ô∏è The following steps should be executed in the **VSCode integrated terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Navigate to Your Directory**\n",
    "Make sure to use this `/jet/home/<your_username>` as your working directory, since all subsequent operations (up to submission) are based on this path.\n",
    "```bash\n",
    "cd /jet/home/<your_username>  #change <your_username> to your username\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Request a Compute Node**\n",
    "```bash\n",
    "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00 -A cis250019p\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Load the Anaconda Module**\n",
    "```bash\n",
    "module load anaconda3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Activate the provided HW1 Environment**\n",
    "```bash\n",
    "conda deactivate # First, deactivate any existing Conda environment\n",
    "conda activate /ocean/projects/cis250019p/mzhang23/TA/HW1P2/envs/hw1p2_env && export PYTHONNOUSERSITE=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Start Jupyter Notebook**\n",
    "Launch Jupyter Notebook:\n",
    "```bash\n",
    "jupyter notebook --no-browser --ip=0.0.0.0\n",
    "```\n",
    "\n",
    "Go to **Kernel** ‚Üí **Select Another Kernel** ‚Üí **Existing Jupyter Server**\n",
    "   Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
    "   \n",
    "   *(Usually, this URL appears in the terminal output after you run `jupyter notebook --no-browser --ip=0.0.0.0`, in a line like:  ‚ÄúJupyter Server is running at: http://...‚Äù)*\n",
    "\n",
    "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n",
    "\n",
    "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Import dependencies**\n",
    "\n",
    "After launching the Jupyter notebook, you can run the cells directly inside the notebook ‚Äî no need to use the terminal for the remaining steps.\n",
    "\n",
    "First, import the dependencies.  \n",
    "(*If you followed the previous steps and correctly activated our shared `hw1p2` environment, you do **not** need to install anything manually.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure you are in your directory\n",
    "!pwd #should be /jet/home/<your_username>, if not, uncomment the following line and replace with your actual username:\n",
    "# %cd <your_username>  #TODO: replace the \"<your_username>\" to yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ **Step 2: Set up Kaggle API Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the same Kaggle code from HW1P2\n",
    "!mkdir /jet/home/<your_username>/.kaggle #TODO: replace the \"<your_username>\" to yours\n",
    "\n",
    "with open(\"/jet/home/<your_username>/.kaggle/kaggle.json\", \"w+\") as f: #TODO: replace the \"<your_username>\" to yours\n",
    "    f.write('{\"username\":\"<YOUR USERNAME>\",\"key\":\"<YOUR KEY>\"}')\n",
    "    # TODO: Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /jet/home/<your_username>/.kaggle/kaggle.json #TODO: replace the \"<your_username>\" to yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ **Step 3: Get Data**\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è The data used in this assignment is **already stored in a shared, read-only folder, so you do not need to manually download anything**.\n",
    "\n",
    "Instead, just make sure to replace the dataset path in your notebook code with the correct path from the shared directory.\n",
    "\n",
    "You can run the following block to explore the shared directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"/ocean/projects/cis250019p/mzhang23/TA/HW1P2/hw1p2_data/11785-f25-hw1p2\" #Shared data path, do not need to change the username to yours\n",
    "print(\"Files in shared hw1p2 dataset:\", os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install tree\n",
    "!tree -L 2 /ocean/projects/cis250019p/mzhang23/TA/HW1P2/hw1p2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-05T16:14:39.970197Z",
     "iopub.status.busy": "2025-09-05T16:14:39.969998Z",
     "iopub.status.idle": "2025-09-05T16:14:45.982228Z",
     "shell.execute_reply": "2025-09-05T16:14:45.981373Z",
     "shell.execute_reply.started": "2025-09-05T16:14:39.970180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:14:45.984501Z",
     "iopub.status.busy": "2025-09-05T16:14:45.984259Z",
     "iopub.status.idle": "2025-09-05T16:16:06.319741Z",
     "shell.execute_reply": "2025-09-05T16:16:06.318872Z",
     "shell.execute_reply.started": "2025-09-05T16:14:45.984478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchaudio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:16:06.320933Z",
     "iopub.status.busy": "2025-09-05T16:16:06.320704Z",
     "iopub.status.idle": "2025-09-05T16:16:17.545328Z",
     "shell.execute_reply": "2025-09-05T16:16:17.544440Z",
     "shell.execute_reply.started": "2025-09-05T16:16:06.320908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import bisect\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "import yaml\n",
    "import torchaudio.transforms as tat\n",
    "import torchaudio\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  \n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:16:17.546440Z",
     "iopub.status.busy": "2025-09-05T16:16:17.546178Z",
     "iopub.status.idle": "2025-09-05T16:16:17.550903Z",
     "shell.execute_reply": "2025-09-05T16:16:17.550308Z",
     "shell.execute_reply.started": "2025-09-05T16:16:17.546411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### PHONEME LIST\n",
    "PHONEMES = [\n",
    "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
    "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
    "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
    "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
    "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
    "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:16:17.573751Z",
     "iopub.status.busy": "2025-09-05T16:16:17.573535Z",
     "iopub.status.idle": "2025-09-05T16:16:17.590853Z",
     "shell.execute_reply": "2025-09-05T16:16:17.590169Z",
     "shell.execute_reply.started": "2025-09-05T16:16:17.573734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Name': 'Jinyao Zhou', # Write your name here\n",
    "    'subset': 1, # Subset of train/val dataset to use (1.0 == 100% of data)\n",
    "    'context': 30,\n",
    "    'archetype': 'pyramid', # Default Values: pyramid, diamond, inverse-pyramid,cylinder\n",
    "    'activations': 'GeLU',\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout': 0.15,\n",
    "    'optimizers': 'AdamW',\n",
    "    'scheduler': 'ReduceLROnPlateau', #ReduceLROnPlateau,CosineAnnealingLR\n",
    "    'T_max': 10,\n",
    "    'eta_min': 0.0002,\n",
    "    'epochs': 30,\n",
    "    'batch_size': 1024,\n",
    "    'weight_decay': 0.001,\n",
    "    'weight_initialization': \"kaiming_normal\", # e.g kaiming_normal, kaiming_uniform, uniform, xavier_normal or xavier_uniform\n",
    "    'augmentations': 'Both', # Options: [\"FreqMask\", \"TimeMask\", \"Both\", null]\n",
    "    'freq_mask_param': 4,\n",
    "    'time_mask_param': 8,\n",
    "    'step_size': 5,\n",
    "    'gamma': 0.99\n",
    "    }\n",
    "\n",
    "run_name = \"<CHANGE TO YOUR WANDB RUN NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:16:17.591819Z",
     "iopub.status.busy": "2025-09-05T16:16:17.591629Z",
     "iopub.status.idle": "2025-09-05T16:16:17.615962Z",
     "shell.execute_reply": "2025-09-05T16:16:17.615326Z",
     "shell.execute_reply.started": "2025-09-05T16:16:17.591804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, phonemes, config, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n",
    "\n",
    "        self.context    = context\n",
    "        self.phonemes   = phonemes\n",
    "        self.subset = config['subset']\n",
    "\n",
    "        # TODO: Initialize augmentations. Read the Pytorch torchaudio documentations on timemasking and frequencymasking\n",
    "        self.freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=config['freq_mask_param'])\n",
    "        self.time_masking = torchaudio.transforms.TimeMasking(time_mask_param=config['time_mask_param'])\n",
    "\n",
    "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
    "        self.mfcc_dir       = root + '/' + partition + '/mfcc'\n",
    "        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
    "        self.transcript_dir = root +'/' + partition + '/transcript'\n",
    "\n",
    "        # TODO: List files in sefl.mfcc_dir using os.listdir in SORTED order\n",
    "        mfcc_names          = os.listdir(self.mfcc_dir)\n",
    "        mfcc_names.sort()\n",
    "        # TODO: List files in self.transcript_dir using os.listdir in SORTED order\n",
    "        transcript_names    = os.listdir(self.transcript_dir)\n",
    "        transcript_names.sort()\n",
    "\n",
    "        # Compute size of data subset\n",
    "        subset_size = int(self.subset * len(mfcc_names))\n",
    "\n",
    "        # Select subset of data to use\n",
    "        mfcc_names = mfcc_names[:subset_size]\n",
    "        transcript_names = transcript_names[:subset_size]\n",
    "\n",
    "        # Making sure that we have the same no. of mfcc and transcripts\n",
    "        assert len(mfcc_names) == len(transcript_names)\n",
    "\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "\n",
    "        # TODO: Iterate through mfccs and transcripts\n",
    "        for i in tqdm(range(len(mfcc_names))):\n",
    "\n",
    "            mfcc             = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n",
    "            mfccs_normalized = (mfcc - np.mean(mfcc, axis=0, keepdims=True)) / np.std(mfcc, axis=0, keepdims=True)\n",
    "            mfccs_normalized = torch.tensor(mfccs_normalized, dtype=torch.float32)\n",
    "\n",
    "            transcript  = np.load(os.path.join(self.transcript_dir, transcript_names[i]), allow_pickle=True).tolist()\n",
    "            transcript  = transcript[1:-1]\n",
    "            transcript_indices = [self.phonemes.index(phoneme) for phoneme in transcript] # map string to number\n",
    "            transcript_indices = torch.tensor(transcript_indices, dtype=torch.int64)\n",
    "\n",
    "            self.mfccs.append(mfccs_normalized)\n",
    "            self.transcripts.append(transcript_indices)\n",
    "\n",
    "        # concatenate along time T\n",
    "        self.mfccs          = torch.cat(self.mfccs, dim=0)\n",
    "        self.transcripts    = torch.cat(self.transcripts, dim=0)\n",
    "        self.length = len(self.mfccs) # len = time duration\n",
    "\n",
    "        self.mfccs = torch.nn.functional.pad(self.mfccs, (0, 0, context, context)) # the trick is you don't need to update the length of mfcc\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = zip(*batch) # unpair x and y one by one\n",
    "        x = torch.stack(x, dim=0)\n",
    "\n",
    "        # agumentation\n",
    "        if np.random.rand() < 0.1: #TODO: changable\n",
    "            x = x.transpose(1, 2)  # Shape: (batch_size, freq, time)\n",
    "            x = self.freq_masking(x)\n",
    "            x = self.time_masking(x)\n",
    "            x = x.transpose(1, 2)  # Shape back to: (batch_size, time, freq)\n",
    "            \n",
    "        return x, torch.tensor(y)\n",
    "\n",
    "    def __getitem__(self, ind):     \n",
    "        frames = self.mfccs[ind:ind+2*self.context+1]\n",
    "        phonemes = self.transcripts[ind]\n",
    "        return frames, phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:16:17.617389Z",
     "iopub.status.busy": "2025-09-05T16:16:17.617099Z",
     "iopub.status.idle": "2025-09-05T16:16:17.638772Z",
     "shell.execute_reply": "2025-09-05T16:16:17.638071Z",
     "shell.execute_reply.started": "2025-09-05T16:16:17.617368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class AudioTestDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
    "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n",
    "    # IMPORTANT: Load complete test data to use, DO NOT select subset of test data, else you will get errors when submitting on Kaggle.\n",
    "\n",
    "    def __init__(self, root, phonemes, config, context=0, partition= \"test-clean\"): # Feel free to add more arguments\n",
    "\n",
    "        self.context    = context\n",
    "        self.phonemes   = phonemes\n",
    "\n",
    "        self.freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=config['freq_mask_param'])\n",
    "        self.time_masking = torchaudio.transforms.TimeMasking(time_mask_param=config['time_mask_param'])\n",
    "\n",
    "        self.mfcc_dir       = root + '/' + partition + '/mfcc'\n",
    "\n",
    "        # TODO: List files in sefl.mfcc_dir using os.listdir in SORTED order\n",
    "        mfcc_names          = os.listdir(self.mfcc_dir)\n",
    "        mfcc_names.sort()\n",
    "\n",
    "        self.mfccs = []\n",
    "\n",
    "        for i in tqdm(range(len(mfcc_names))):\n",
    "            mfcc             = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n",
    "            mfccs_normalized = (mfcc - np.mean(mfcc, axis=0, keepdims=True)) / np.std(mfcc, axis=0, keepdims=True)\n",
    "            mfccs_normalized = torch.tensor(mfccs_normalized, dtype=torch.float32)\n",
    "            self.mfccs.append(mfccs_normalized)\n",
    "\n",
    "        self.mfccs = torch.cat(self.mfccs, dim=0)\n",
    "        self.length = len(self.mfccs)\n",
    "        self.mfccs = torch.nn.functional.pad(self.mfccs, (0, 0, context, context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        frames = self.mfccs[ind:ind+2*self.context+1]\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:16:17.639773Z",
     "iopub.status.busy": "2025-09-05T16:16:17.639484Z",
     "iopub.status.idle": "2025-09-05T16:17:29.905936Z",
     "shell.execute_reply": "2025-09-05T16:17:29.905085Z",
     "shell.execute_reply.started": "2025-09-05T16:16:17.639747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ROOT = \"/kaggle/input/11785-hw-1-p-2-fall-2025-main/11785-f25-hw1p2/\" # Define the root directory of the dataset here\n",
    "ROOT = data_path\n",
    "train_data = AudioDataset(ROOT, PHONEMES, config, context=config['context'])\n",
    "val_data = AudioDataset(ROOT, PHONEMES, config, context=config['context'], partition=\"dev-clean\")\n",
    "test_data = AudioTestDataset(ROOT, PHONEMES, config, context=config['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:29.907188Z",
     "iopub.status.busy": "2025-09-05T16:17:29.906953Z",
     "iopub.status.idle": "2025-09-05T16:17:29.914616Z",
     "shell.execute_reply": "2025-09-05T16:17:29.913692Z",
     "shell.execute_reply.started": "2025-09-05T16:17:29.907172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define dataloaders for train, val and test datasets\n",
    "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
    "# We shuffle train dataloader but not val & test dataloader. Why?\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    num_workers = 4, #change for differnet device\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True,\n",
    "    collate_fn = train_data.collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    num_workers = 0,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test_data,\n",
    "    num_workers = 0,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Batch size     : \", config['batch_size'])\n",
    "print(\"Context        : \", config['context'])\n",
    "print(\"Input size     : \", (2*config['context']+1)*28)\n",
    "print(\"Output symbols : \", len(PHONEMES))\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:29.915863Z",
     "iopub.status.busy": "2025-09-05T16:17:29.915590Z",
     "iopub.status.idle": "2025-09-05T16:17:31.298687Z",
     "shell.execute_reply": "2025-09-05T16:17:31.297956Z",
     "shell.execute_reply.started": "2025-09-05T16:17:29.915823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Testing code to check if your data loaders are working\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     frames, phoneme = data\n",
    "#     print(frames.shape, phoneme.shape)\n",
    "\n",
    "#     # Visualize sample mfcc to inspect and verify everything is correctly done, especially augmentations\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.imshow(frames[0].numpy().T, aspect='auto', origin='lower', cmap='viridis')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Features')\n",
    "#     plt.title('Feature Representation')\n",
    "#     plt.show()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:31.300340Z",
     "iopub.status.busy": "2025-09-05T16:17:31.299790Z",
     "iopub.status.idle": "2025-09-05T16:17:31.330310Z",
     "shell.execute_reply": "2025-09-05T16:17:31.329548Z",
     "shell.execute_reply.started": "2025-09-05T16:17:31.300313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Testing code to check if your validation data loaders are working\n",
    "all = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    frames, phoneme = data\n",
    "    all.append(phoneme)\n",
    "    print(frames.shape, phoneme.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:31.331667Z",
     "iopub.status.busy": "2025-09-05T16:17:31.331407Z",
     "iopub.status.idle": "2025-09-05T16:17:31.340300Z",
     "shell.execute_reply": "2025-09-05T16:17:31.339582Z",
     "shell.execute_reply.started": "2025-09-05T16:17:31.331648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, config):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.weight_init = config[\"weight_initialization\"]\n",
    "        self.dropout = config[\"dropout\"]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),      \n",
    "            nn.BatchNorm1d(2048),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout*1.25),\n",
    "\n",
    "            nn.Linear(2048, 2048),            \n",
    "            nn.BatchNorm1d(2048),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout*1.25),\n",
    "\n",
    "            nn.Linear(2048, 1536),            \n",
    "            nn.BatchNorm1d(1536),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "\n",
    "            nn.Linear(1536, 1024),            \n",
    "            nn.BatchNorm1d(1024),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "\n",
    "            nn.Linear(1024, 1024),            \n",
    "            nn.BatchNorm1d(1024),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "\n",
    "            nn.Linear(1024, 512),            \n",
    "            nn.BatchNorm1d(512),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "\n",
    "            nn.Linear(512, 512),            \n",
    "            nn.BatchNorm1d(512),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "\n",
    "            nn.Linear(512, 256),           \n",
    "            nn.BatchNorm1d(256),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout*0.5),\n",
    "\n",
    "            nn.Linear(256, 256),            \n",
    "            nn.BatchNorm1d(256),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=self.dropout*0.5),\n",
    "\n",
    "            nn.Linear(256, 128),            \n",
    "            nn.BatchNorm1d(128),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(p=self.dropout),\n",
    "\n",
    "            nn.Linear(128, output_size)      \n",
    "        )\n",
    "\n",
    "        if self.weight_init is not None:\n",
    "            self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if self.weight_init == \"xavier_normal\":\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                elif self.weight_init == \"xavier_uniform\":\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                elif self.weight_init == \"kaiming_normal\":\n",
    "                    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                elif self.weight_init == \"kaiming_uniform\":\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                elif self.weight_init == \"uniform\":\n",
    "                    nn.init.uniform_(m.weight)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid weight_initialization value\")\n",
    "\n",
    "                # Initialize bias to 0\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten to a 1D vector for each data point\n",
    "        x = torch.flatten(x, start_dim=1)  # Keeps batch size, flattens the rest\n",
    "\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:31.344199Z",
     "iopub.status.busy": "2025-09-05T16:17:31.343987Z",
     "iopub.status.idle": "2025-09-05T16:17:31.462361Z",
     "shell.execute_reply": "2025-09-05T16:17:31.461699Z",
     "shell.execute_reply.started": "2025-09-05T16:17:31.344182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the input size\n",
    "INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\n",
    "\n",
    "# Instantiate model and load to GPU\n",
    "model       = Network(INPUT_SIZE, len(train_data.phonemes), config).to(device).cuda()\n",
    "\n",
    "# Remember, you are limited to 20 million parameters for HW1 (including ensembles)\n",
    "# Check to stay below 20 MIL Parameter limit\n",
    "assert sum(p.numel() for p in model.parameters() if p.requires_grad) < 20_000_000, \"Exceeds 20 MIL params. Any submission made to Kaggle with this model will be flagged as an AIV.\"\n",
    "\n",
    "# .pth checkpoint file path can also be obtained from a locally saved .pth file. Or, you can use the checkpoint_dict obtained from the prior wandb artifact download :)\n",
    "checkpoint_path = \"<PATH TO YOUR CHAECKPOINT>.pth\"\n",
    "checkpoint_dict = torch.load(checkpoint_path)\n",
    "\n",
    "# Loading model weights\n",
    "# if isinstance(model, nn.DataParallel) model.module.load_state_dict(checkpoint_dict['model_state_dict'])\n",
    "model.load_state_dict(checkpoint_dict['model_state_dict'])\n",
    "# from torchsummaryX import summary\n",
    "\n",
    "# summary(model, frames.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'],weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='max',factor=0.4, patience=5,threshold=0.0005) \n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
    "\n",
    "# Loading optimizer state\n",
    "optimizer.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
    "# Loading the scheduler state\n",
    "scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:31.463394Z",
     "iopub.status.busy": "2025-09-05T16:17:31.463122Z",
     "iopub.status.idle": "2025-09-05T16:17:35.024272Z",
     "shell.execute_reply": "2025-09-05T16:17:35.023307Z",
     "shell.execute_reply.started": "2025-09-05T16:17:31.463368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inspect model architecture and check to verify number of parameters of your network\n",
    "# try:\n",
    "#     # Install and import torchsummaryX\n",
    "#     !pip install torchsummaryX==1.1.0\n",
    "#     from torchsummaryX import summary\n",
    "\n",
    "#     summary(model, frames.to(device))\n",
    "\n",
    "# except:\n",
    "#     !pip install torchsummary\n",
    "#     from torchsummary import summary\n",
    "\n",
    "#     summary(model, frames[0].to(device).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:38.260146Z",
     "iopub.status.busy": "2025-09-05T16:17:38.259547Z",
     "iopub.status.idle": "2025-09-05T16:17:38.505128Z",
     "shell.execute_reply": "2025-09-05T16:17:38.504377Z",
     "shell.execute_reply.started": "2025-09-05T16:17:38.260117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CLEAR RAM!!\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:38.506142Z",
     "iopub.status.busy": "2025-09-05T16:17:38.505887Z",
     "iopub.status.idle": "2025-09-05T16:17:38.512915Z",
     "shell.execute_reply": "2025-09-05T16:17:38.512371Z",
     "shell.execute_reply.started": "2025-09-05T16:17:38.506117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
    "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "\n",
    "        ### Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        frames      = frames.to(device)\n",
    "        phonemes    = phonemes.to(device)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.float16): # bfloat16 for mac, change back to float16 for other devices\n",
    "            ### Forward Propagation\n",
    "            logits  = model(frames)\n",
    "\n",
    "            ### Loss Calculation\n",
    "            loss    = criterion(logits, phonemes)\n",
    "\n",
    "        ### Backward Propagation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # OPTIONAL: You can add gradient clipping here, if you face issues of exploding gradients\n",
    "\n",
    "        ### Gradient Descent\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        tloss   += loss.item()\n",
    "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    batch_bar.close()\n",
    "    tloss   /= len(dataloader)\n",
    "    tacc    /= len(dataloader)\n",
    "\n",
    "\n",
    "    return tloss, tacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:38.513933Z",
     "iopub.status.busy": "2025-09-05T16:17:38.513660Z",
     "iopub.status.idle": "2025-09-05T16:17:38.535111Z",
     "shell.execute_reply": "2025-09-05T16:17:38.534551Z",
     "shell.execute_reply.started": "2025-09-05T16:17:38.513908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def eval(model, dataloader, device, criterion):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
    "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "\n",
    "        ### Move data to device (ideally GPU)\n",
    "        frames      = frames.to(device)\n",
    "        phonemes    = phonemes.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode():\n",
    "            ### Forward Propagation\n",
    "            logits  = model(frames)\n",
    "            ### Loss Calculation\n",
    "            loss    = criterion(logits, phonemes)\n",
    "\n",
    "        vloss   += loss.item()\n",
    "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
    "\n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
    "                                acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss   /= len(dataloader)\n",
    "    vacc    /= len(dataloader)\n",
    "\n",
    "    return vloss, vacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:17:44.263982Z",
     "iopub.status.busy": "2025-09-05T16:17:44.263599Z",
     "iopub.status.idle": "2025-09-05T16:17:51.154825Z",
     "shell.execute_reply": "2025-09-05T16:17:51.154164Z",
     "shell.execute_reply.started": "2025-09-05T16:17:44.263965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def wandb_run(model, config, name: str, resume_old_run=False, id=None):\n",
    "\n",
    "    wandb.login(key=\"<YOUR KEY>\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n",
    "\n",
    "    # Create your wandb run\n",
    "    RESUME_OLD_RUN =resume_old_run\n",
    "\n",
    "    if RESUME_OLD_RUN == True:\n",
    "        print(\"Resuming previous WanDB run...\")\n",
    "        run = wandb.init(\n",
    "            name    = name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            id     = id, ### TODO: Insert specific run id here if you want to resume a previous run\n",
    "            resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "            project = \"<YOUR PROJECT NAME>\", ### Project should be created in your wandb account\n",
    "            config  = config ### Wandb Config for your run\n",
    "        )\n",
    "    else:\n",
    "        print(\"Initializing new WanDB run...\")\n",
    "        run = wandb.init(\n",
    "            name    = name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = \"<YOUR PROJECT NAME>\", ### Project should be created in your wandb account\n",
    "            config  = config ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "    ### Save your model architecture as a string with str(model)\n",
    "    model_arch  = str(model)\n",
    "    arch_filename = f\"<YOUR PROJECT NAME>_{name}.txt\"  # Âü∫‰∫énameÂèÇÊï∞ÁîüÊàêÊñá‰ª∂Âêç\n",
    "    arch_file   = open(arch_filename, \"w\")\n",
    "    file_write  = arch_file.write(model_arch)\n",
    "    arch_file.close()\n",
    "\n",
    "    ### log it in your wandb run with wandb.save()\n",
    "    wandb.save(arch_filename)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:19:39.758208Z",
     "iopub.status.busy": "2025-09-05T16:19:39.757481Z",
     "iopub.status.idle": "2025-09-05T16:24:19.090487Z",
     "shell.execute_reply": "2025-09-05T16:24:19.089642Z",
     "shell.execute_reply.started": "2025-09-05T16:19:39.758183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Iterate over number of epochs to train and evaluate your model\n",
    "\n",
    "# run = wandb_run(model, config, run_name)\n",
    "run = wandb_run(model, config, run_name, resume_old_run=True, id=\"<ID OF WANDB RUN YOU WANT TO RECOVER>\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion, device, scaler)\n",
    "    val_loss, val_acc       = eval(model, val_loader, device, criterion)\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
    "\n",
    "    ## Log metrics at each epoch in your run\n",
    "    # Optionally, you can log at each batch inside train/eval functions\n",
    "    # (explore wandb documentation/wandb recitation)\n",
    "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
    "            'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
    "\n",
    "    # If using a scheduler, step the learning rate here, otherwise comment this line\n",
    "    # Depending on the scheduler in use, you may or may not need to pass in a metric into the step function, so read the docs well\n",
    "    \n",
    "    if config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        scheduler.step(val_acc)\n",
    "    else:\n",
    "        scheduler.step() ## or (val_acc) for StepLR\n",
    "    \n",
    "    ## HIGHLY RECOMMENDED: Save model checkpoint in drive and/or wandb if accuracy is better than your current best accuracy\n",
    "    ## This enables you to resume training at anytime, without having to start from scratch.\n",
    "    ## Refer to Recitation 0.24 to learn how to implement this: https://www.youtube.com/watch?v=-TCH0DbUEKM&list=PLp-0K3kfddPw2D5CeA09lsx_oNy9E\n",
    "    \n",
    "    # Save checkpoint only when validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        patience = 0;\n",
    "        best_val_acc = val_acc\n",
    "        checkpoint_path = f\"{run_name}_best.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'config': config\n",
    "        }, checkpoint_path)\n",
    "        # Save to wandb\n",
    "        wandb.save(checkpoint_path)\n",
    "        print(f\"üéâ New best model saved! Val Acc: {val_acc:.4f} -> {checkpoint_path}\")\n",
    "    else:\n",
    "        patience += 1;\n",
    "        print(f\"Val Acc: {val_acc:.4f} (Best: {best_val_acc:.4f}) - No improvement, patience={patience}\")\n",
    "\n",
    "    \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:26:47.033810Z",
     "iopub.status.busy": "2025-09-05T16:26:47.033039Z",
     "iopub.status.idle": "2025-09-05T16:26:47.041336Z",
     "shell.execute_reply": "2025-09-05T16:26:47.040564Z",
     "shell.execute_reply.started": "2025-09-05T16:26:47.033780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, PHONEMES, test_loader, device):\n",
    "    ### What you call for model to perform inference?\n",
    "    model.eval() # TODO train or eval?\n",
    "\n",
    "    ### List to store predicted phonemes of test data\n",
    "    test_predictions = []\n",
    "\n",
    "    ### Which mode do you need to avoid gradients?\n",
    "    with torch.no_grad(): #TODO\n",
    "\n",
    "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
    "\n",
    "            mfccs   = mfccs.to(device)\n",
    "\n",
    "            logits  = model(mfccs)\n",
    "\n",
    "            ### Get most likely predicted phoneme with argmax\n",
    "            predicted_phonemes = torch.argmax(logits, dim=1)\n",
    "\n",
    "            ### How do you store predicted_phonemes with test_predictions? HINT: look at the eval() function from earlier\n",
    "            # Remember the phonemes were converted from strings to their corresponding integer indices earlier, and the results of the argmax is a list of the integer indices of the predicted phonemes.\n",
    "            # So how do you get and store the actual predicted phonemes (strings NOT integers)\n",
    "            # TODO: Convert predicted_phonemes (integer indices from argmax) back to phoneme strings and append them to test_predictions\n",
    "            for phoneme_index in predicted_phonemes:\n",
    "                test_predictions.append(PHONEMES[phoneme_index])\n",
    "\n",
    "            # raise NotImplementedError(\n",
    "            #     \"TODO: convert predicted_phonemes integer indices -> phoneme strings and append to test_predictions. \"\n",
    "            #     \"Replace this exception with the correct code implementation.\"\n",
    "            # )\n",
    "\n",
    "    ## SANITY CHECK\n",
    "    sample_predictions = test_predictions[:10]\n",
    "    if not isinstance(sample_predictions[0], str):\n",
    "        print(f\"‚ùå ERROR: Predictions should be phoneme STRINGS, not {type(sample_predictions[0]).__name__}!\")\n",
    "        print(f\"   You need to convert integer indices to their corresponding phoneme strings\")\n",
    "        print(f\"   Hint: Look at the eval() function to get the idea\")\n",
    "\n",
    "    # Print a preview of predictions for manual inspection\n",
    "    print(\"\\nSample predictions:\", sample_predictions)\n",
    "    print(\"\\nPredictions Generated successfully!\")\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:26:49.953103Z",
     "iopub.status.busy": "2025-09-05T16:26:49.952391Z",
     "iopub.status.idle": "2025-09-05T16:27:31.271121Z",
     "shell.execute_reply": "2025-09-05T16:27:31.270277Z",
     "shell.execute_reply.started": "2025-09-05T16:26:49.953077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate model test predictions\n",
    "\n",
    "predictions = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T16:29:59.296979Z",
     "iopub.status.busy": "2025-09-05T16:29:59.296699Z",
     "iopub.status.idle": "2025-09-05T16:30:00.328210Z",
     "shell.execute_reply": "2025-09-05T16:30:00.327581Z",
     "shell.execute_reply.started": "2025-09-05T16:29:59.296963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Create CSV file with predictions\n",
    "\n",
    "with open(\"./submission.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(\"{},{}\\n\".format(i, predictions[i]))\n",
    "\n",
    "    print(\"submission.csv file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13550858,
     "sourceId": 113776,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "idlf25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
